{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Aimes, Iowa Housing Data: Preprocessing and Model Fitting (Intuitive Interactions, Linear Regression)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we process our data further before fitting it into a model.  The data is split into training and testing sets, scaled and fit.  Predictors are guessed through previous EDA and fitting more area variables, though these are admittedly guessed toward.  We also note that there was skew in the distributions, but a linear regression is used in this manner to interpret large scale trends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import patsy\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "import pickle\n",
    "\n",
    "np.random.seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fileObj = open('./pickles/housingDF.pkl', 'rb')\n",
    "housing = pickle.load(fileObj)\n",
    "fileObj.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we further explore modelling on a minimal set of interactions.  Lot frontage may have had a more obvious interaction with lot configuration, but these were discarded earlier due to missing information.  Imputation may have had biased results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = 'TotalBsmtSF:BsmtExposure + LotArea:LotConfig + GarageArea:GarageFinish + OverallQual:GrLivArea'\n",
    "formula = f'SalePrice ~ {interactions} - 1'\n",
    "y, x = patsy.dmatrices(formula, housing)\n",
    "x = pd.DataFrame(x, columns=x.design_info.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.7, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "xtest = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's R^2 score is 0.896104532472778 on the training data, 0.8530487255962007 on the test data.\n"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain)\n",
    "print(f'The model\\'s R^2 score is {model.score(xtrain, ytrain)} on the training data, {model.score(xtest, ytest)} on the test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30125.558788029295"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_squared_error(ytest, model.predict(xtest))**0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This model scores somewhat consistently in cross validation, and the largest weight is from quality area(OverallQual:GrLivArea), as promised.  We see that garage area has a large negative coefficient for garages of type 0(those that were listed as having no garage) to conform to the data, but making no sense if we try to infer a relationship from this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OverallQual:GrLivArea           4.674827e+04\n",
       "GarageArea:GarageFinish[Fin]    2.352433e+04\n",
       "TotalBsmtSF:BsmtExposure[Gd]    2.302346e+04\n",
       "GarageArea:GarageFinish[RFn]    2.038898e+04\n",
       "TotalBsmtSF:BsmtExposure[Av]    1.326832e+04\n",
       "TotalBsmtSF:BsmtExposure[No]    1.168230e+04\n",
       "GarageArea:GarageFinish[Unf]    8.867471e+03\n",
       "TotalBsmtSF:BsmtExposure[Mn]    7.189679e+03\n",
       "LotArea:LotConfig[Inside]       4.635108e+03\n",
       "LotArea:LotConfig[CulDSac]      2.550493e+03\n",
       "LotArea:LotConfig[Corner]       1.698476e+03\n",
       "LotArea:LotConfig[FR2]          9.548840e+02\n",
       "TotalBsmtSF:BsmtExposure[0]     0.000000e+00\n",
       "GarageArea:GarageFinish[0]     -9.094947e-13\n",
       "LotArea:LotConfig[FR3]         -1.684410e+03\n",
       "dtype: float64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(model.coef_[0], index=x.columns).sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 folds, the R^2 score is 0.8768389605082882 +- 0.037195135717457986\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, xtrain, ytrain.ravel(), cv=10)\n",
    "print(f'With 10 folds, the R^2 score is {np.mean(scores)} +- {np.std(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This may be just by virtue of poorly choosing parameters: in fact, we get similar results by applying a linear combination of overall quality and living area rather than the product."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "interactions = 'TotalBsmtSF:BsmtExposure + LotArea:LotConfig + GarageArea:GarageFinish + OverallQual + GrLivArea'\n",
    "formula = f'SalePrice ~ {interactions} - 1'\n",
    "y, x = patsy.dmatrices(formula, housing)\n",
    "x = pd.DataFrame(x, columns=x.design_info.column_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LinearRegression()\n",
    "xtrain, xtest, ytrain, ytest = train_test_split(x, y, test_size=0.7, shuffle=True)\n",
    "\n",
    "scaler = StandardScaler()\n",
    "xtrain = scaler.fit_transform(xtrain)\n",
    "xtest = scaler.transform(xtest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model's R^2 score is 0.8686897681690234 on the training data, 0.8430454758047368 on the test data.\n"
     ]
    }
   ],
   "source": [
    "model.fit(xtrain, ytrain)\n",
    "print(f'The model\\'s R^2 score is {model.score(xtrain, ytrain)} on the training data, {model.score(xtest, ytest)} on the test data.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With 10 folds, the R^2 score is 0.8518301924838829 +- 0.030771909533508765\n"
     ]
    }
   ],
   "source": [
    "scores = cross_val_score(model, xtrain, ytrain.ravel(), cv=10)\n",
    "print(f'With 10 folds, the R^2 score is {np.mean(scores)} +- {np.std(scores)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than guessing for features, we should automate the process.  We first try this using a Lasso regression model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
